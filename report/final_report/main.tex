\documentclass[12pt,a4paper,fleqn]{report}
\usepackage[left=35mm,right=25mm,top=25mm,bottom=25mm]{geometry}
\usepackage[sort&compress,square,numbers]{natbib}
\usepackage{bookmark}
\usepackage{url}
\usepackage{hyperref}
\usepackage{svg}
\hypersetup{
    colorlinks=false, %set true if you want colored links
    linktoc=all,     %set to all if you want both sections and subsections linked
    linkcolor=blue,  %choose some color if you want links to stand out
}
\usepackage{float}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{minted}
\setmintedinline{breaklines}
% \usepackage[table,xcdraw]{xcolor}
\usepackage[labelfont=bf,justification=centering]{caption, subcaption}

\title{Real-time Vehicle Counting using OpenCV}
\author{Mark Cai Yee Lee}
\date{March 2022}

\begin{document}
\maketitle

\chapter{Introduction}

Traffic density is projected to grow along with the human population. 
According to \cite{placek:2022}, the number of cars produced worldwide has increased from 58 million
units from 2000 to 97 million units in 2018.
In time, this growth must be managed to plan and design traffic facilities, assist traffic
operation, evaluate road safety and define general traffic laws.
In Australia, traffic density is typically monitored using pneumatic tubing and inductive loop
sensors. 
These counting methods are expensive to deploy and cannot accurately classify and count vehicles in
real-time.

\section{Aim}
The aim of this project is to improve upon the project in \cite{}. 
The project in \cite{} utilises traditional computer vision to count vehicles in real-time. 
In this case, this project will focus on counting vehicles in real-time in day and nighttime
conditions. 
The project also aim to find the best accuracy and processing power trade-off for counting
vehicles using deep learning.

\section{Background}

\subsection{You Only Look Once (YOLO)}
YOLO is a one-stage object detector.
The current versions of YOLO are YOLOv3, YOLOv4 \cite{yolov4:2020} and YOLOv5.
As reported in \cite{compareyolo345:2022}, YOLOv5l is able to achieve a Mean Average Precision (MAP) of 0.633 as compared to 0.46 and 0.607 of YOLOv3 and YOLOv4 respectively. 
In addition, YOLOv5 has the most variations which can be used according to the processing requirements of a
project.
This includes YOLOv5x, YOLOv5l, YOLOv5m, YOLOv5s and YOLOv5n \cite{yolov5git:2022}.
The variations of YOLOv5 have the same architecture. 
However, the complexity of the convolutional neural network (CNN) is scaled using compound scaling
\cite{efficentdet:2019,efficientnet:2019}. 
In this case, the width and depth of the neural network is scaled to produce deep learning models
with varying degrees of computational requirements.
Table \ref{tab:yolov5comp} shows the differences between each version of YOLOv5 at release version 6.1. 

\begin{table}[htbp]
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\textbf{Model}} & \textbf{\begin{tabular}[c]{@{}c@{}}size\\ (pixels)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}mAPval\\ 0.5:0.95\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}mAPval\\ 0.5\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Speed\\ CPU b1\\ (ms)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}params\\ (M)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}FLOPs\\ @640 (B)\end{tabular}} \\ \hline
\textbf{YOLOv5n} & 640 & 28.0 & 45.7 & 45 & 1.9 & 4.5 \\ \hline
\textbf{YOLOv5s} & 640 & 37.4 & 56.8 & 98 & 7.2 & 16.5 \\ \hline
\textbf{YOLOv5m} & 640 & 45.4 & 64.1 & 224 & 21.2 & 49.0 \\ \hline
\textbf{YOLOv5l} & 640 & 49.0 & 67.3 & 430 & 46.5 & 109.1 \\ \hline
\textbf{YOLOv5x} & 640 & 50.7 & 68.9 & 766 & 86.7 & 205.7 \\ \hline
\end{tabular}
\caption{Comparison between Variations of YOLOv5 Release 6.1 \cite{yolov5git:2022}}
\label{tab:yolov5comp}
\end{table}

\subsubsection{Architecture}
The architecture of YOLOv5 has three main components, the backbone, the neck and the head.
YOLOv5 uses a modified version of CSP-Darknet53 \cite{} as the backbone.
A CNN backbone is a feature extractor which 

\subsection{Simple, Online, Real-time Tracking (SORT)}

\chapter{Research Methodology}

\section{System Design}

\subsection{Detection}

\subsection{Tracking}

\subsection{Counting}

\subsection{Interfacing}

\section{Experiment Methodology}
The experiment was conducted in two locations both of which are above a highway. 
Video footage of both locations are recorded using a Pixel 4 at 1080p 30FPS. 
The Google Pixel 4 was secured using a tripod and put near the railing of the bridge for stability.
This is shown in Fig. \ref{}.
The parameters for each video are shown in Table \ref{}.

The experiment uses the Open Broadcaster Software (OBS) to simulate accessing the camera on-site. 
OBS can generate a virtual camera which can be accessed using OpenCV.
This is because the \mintinline{c++}{VideoCapture.read()} function does not read video frames according to time.
As presented in the example shown below, this program will wait 1 ms at best effort before reading
each frame.
However, the program will be able to finish reading the video faster than actual time.

\definecolor{bg}{rgb}{0.95,0.95,0.95}
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos,
]{c++}
#include <opencv2/core.hpp>

VideoCapture cap;
Mat frame;
while(cap.empty() != true){ 
    cap >> frame; 
    
    imshow("My Video", frame);
    waitKey(1);
\end{minted}

This project investigates the accuracy of vehicle counting using two variations of YOLOv5 and SORT
algorithm. 
This project also investigates the accuracy of nighttime vehicle counting. 
The parameters used for obtaining measurements are shown in Tab. \ref{}.

\chapter{Results and Analysis}

\chapter{Conclusion and Future Work}
\bibliographystyle{unsrtnat}

\bibliography{references}{}

\end{document}
